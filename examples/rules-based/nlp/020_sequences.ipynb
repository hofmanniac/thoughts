{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Sequences\n",
    "\n",
    "Natural language is encoded in patterns, which are themselves comprised of sequences of tokens. Therefore, we want to be able to write rules that can detect these sequences.\n",
    "\n",
    "For example, noun phrases are often constructed with a determiner*, such as \"a\" or \"the\", followed by an optional sequence of adjectives, followed by a noun. This is just one possible pattern for noun phrases, but useful for illustration purposes here.\n",
    "\n",
    "*In some texts a \"determiner\" is also known as an \"article\", and some would consider it to be another type of adjective. We're considering it as a separate category here to help illustrate different ways that you can build up sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, os.path.abspath('..\\\\..\\\\..'))\n",
    "from thoughts.rules_engine import RulesEngine\n",
    "import pprint\n",
    "\n",
    "# start a new engine\n",
    "engine = RulesEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Tokenizing Text and Enhancing the Tokens\n",
    "\n",
    "In the last tutorial on Tokens, we generated tokens from a sentence and then used the #lookup command to enhance those tokens with information from the KB. Let's run that code now to catch up with the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lemma': 'the', 'cat': 'det', '#seq-start': 0, '#seq-end': 1},\n",
      " {'lemma': 'quick', 'cat': 'adj', '#seq-start': 1, '#seq-end': 2},\n",
      " {'lemma': 'fox', 'cat': 'noun', '#seq-start': 2, '#seq-end': 3},\n",
      " {'lemma': 'jumped', 'cat': 'verb', '#seq-start': 3, '#seq-end': 4},\n",
      " {'lemma': 'over', 'cat': 'prep', '#seq-start': 4, '#seq-end': 5},\n",
      " {'lemma': 'the', 'cat': 'det', '#seq-start': 5, '#seq-end': 6},\n",
      " {'lemma': 'lazy', 'cat': 'adj', '#seq-start': 6, '#seq-end': 7},\n",
      " {'lemma': 'dog', 'cat': 'noun', '#seq-start': 7, '#seq-end': 8}]\n"
     ]
    }
   ],
   "source": [
    "rules = [\n",
    "  \n",
    "    {\"lemma\": \"the\", \"cat\": \"det\"},\n",
    "    {\"lemma\": \"quick\", \"cat\": \"adj\"},\n",
    "    {\"lemma\": \"brown\", \"cat\": \"adj\"},\n",
    "    {\"lemma\": \"fox\", \"cat\": \"noun\"},\n",
    "    {\"lemma\": \"jumped\", \"cat\": \"verb\"},\n",
    "    {\"lemma\": \"over\", \"cat\": \"prep\"},\n",
    "    {\"lemma\": \"lazy\", \"cat\": \"adj\"},\n",
    "    {\"lemma\": \"dog\", \"cat\": \"noun\"},\n",
    "\n",
    "    {\"#when\": {\"text\": \"?sentence\"},\n",
    "     \"#then\": [{\"#tokenize\": \"?sentence\", \"assert\": {\"#lookup\": {\"lemma\": \"#\"}}}]\n",
    "    }\n",
    "]\n",
    "\n",
    "engine.add_rules(rules)\n",
    "\n",
    "results = engine.process({\"text\": \"the quick fox jumped over the lazy dog\"})\n",
    "pprint.pprint(results, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-Based Rules\n",
    "\n",
    "Let's write a rule to detect the sequence where the \"cat\" (category) is det-adj-noun. The #when clause supports this by wrapping the terms in a list: \"#when\": [{term1}, {term2}, {term3}].\n",
    "\n",
    "When the engine detects the first term (term1), it will hold on to that rule in memory as an Arc. An arc is a rule which is in the process of being matched, waiting for the next term (term2) to arrive. Each time a new term arrives via assertion and is matched to an arc, the arc is then copied and advanced forward one term. In this way, the engine hangs on to all partial matches along the way during parsing. If you are familiar with chart parsing, this is the same concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lemma': 'the', 'cat': 'det', '#seq-start': 0, '#seq-end': 1},\n",
      " {'lemma': 'quick', 'cat': 'adj', '#seq-start': 1, '#seq-end': 2},\n",
      " {'cat': 'np', 'entity': 'fox'},\n",
      " {'lemma': 'jumped', 'cat': 'verb', '#seq-start': 3, '#seq-end': 4},\n",
      " {'lemma': 'over', 'cat': 'prep', '#seq-start': 4, '#seq-end': 5},\n",
      " {'lemma': 'the', 'cat': 'det', '#seq-start': 5, '#seq-end': 6},\n",
      " {'lemma': 'lazy', 'cat': 'adj', '#seq-start': 6, '#seq-end': 7},\n",
      " {'cat': 'np', 'entity': 'dog'}]\n"
     ]
    }
   ],
   "source": [
    "rules = [\n",
    "    {\"#when\": [{\"cat\": \"det\"}, {\"cat\": \"adj\"}, {\"cat\": \"noun\", \"lemma\": \"?noun\"}],\n",
    "    \"#then\": [{\"cat\": \"np\", \"entity\": \"?noun\"}]}\n",
    "]\n",
    "\n",
    "engine.add_rules(rules)\n",
    "\n",
    "results = engine.process({\"text\": \"the quick fox jumped over the lazy dog\"})\n",
    "pprint.pprint(results, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearing the Arcs\n",
    "\n",
    "As described above, the engine hangs on to all partial matches for rules as arcs. Arcs are partially matched rules waiting for the next term to arrive.\n",
    "\n",
    "You as a rule developer then need to decide when the engine should clear out its arcs. The most common time to clear them out is immediately before the next full parse. You can do this with the #clear-arcs command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lemma': 'the', 'cat': 'det', '#seq-start': 0, '#seq-end': 1},\n",
      " {'lemma': 'quick', 'cat': 'adj', '#seq-start': 1, '#seq-end': 2},\n",
      " {'cat': 'np', 'entity': 'fox'},\n",
      " {'lemma': 'jumped', 'cat': 'verb', '#seq-start': 3, '#seq-end': 4},\n",
      " {'lemma': 'over', 'cat': 'prep', '#seq-start': 4, '#seq-end': 5},\n",
      " {'lemma': 'the', 'cat': 'det', '#seq-start': 5, '#seq-end': 6},\n",
      " {'lemma': 'lazy', 'cat': 'adj', '#seq-start': 6, '#seq-end': 7},\n",
      " {'cat': 'np', 'entity': 'dog'}]\n"
     ]
    }
   ],
   "source": [
    "## manually clearing the arcs via code\n",
    "## you can also do this with {\"#clear-arcs\": \"\"} in a command in the #then clause:\n",
    "# {\"#when\": {\"text\": \"?sentence\"},\n",
    "#  \"#then\": [{\"#clear-arcs\": \"\"},\n",
    "#            {\"#tokenize\": \"?sentence\", \"assert\": {\"#lookup\": {\"lemma\": \"#\"}}}]\n",
    "# }\n",
    "engine.clear_arcs()\n",
    "\n",
    "results = engine.process({\"text\": \"the quick fox jumped over the lazy dog\"})\n",
    "pprint.pprint(results, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next - Structures\n",
    "\n",
    "Now that we've seen how to build sequences, let's look at how to build up structures, which can detect multiple sequences at different levels and build them up into assertion facts for additional rules to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
