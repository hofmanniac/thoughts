{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Sentences Using Tokens\n",
    "\n",
    "Template-based parsing from the last tutorial is a useful and powerful technique for many natural language processing scenarios, but also has its limitations.\n",
    "\n",
    "What if you could analyze the sentence word-by-word, using the information in each word to contribute to the overall meaning of the sentence? This is the technique used by most modern day parsing techniques, including those used in the machine learning and symbolic computing communities.\n",
    "\n",
    "Let's start up the engine and walk through an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, os.path.abspath('..\\\\..'))\n",
    "from thoughts.rules_engine import RulesEngine\n",
    "import pprint\n",
    "\n",
    "# start a new engine\n",
    "engine = RulesEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Input\n",
    "\n",
    "The first step is to \"tokenize\" the input. We need a way to take a raw sentence, which is a sequence of words, and to trigger rules for each individual word in the sentence.\n",
    "\n",
    "To do this, we can use the #tokenize command. This command will assert a fact for every word in an input string. In the example below, the ?sentence input text is used in the #tokenize command. The command will output an assertion for every word in the ?sentence variable.\n",
    "\n",
    "Note that only the final set of resulting assertions is displayed in the output. These are the tokenized assertions generated by the #tokenize command from the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'#': 'the', '#seq-start': 0, '#seq-end': 1},\n",
      " {'#': 'quick', '#seq-start': 1, '#seq-end': 2},\n",
      " {'#': 'brown', '#seq-start': 2, '#seq-end': 3},\n",
      " {'#': 'fox', '#seq-start': 3, '#seq-end': 4},\n",
      " {'#': 'jumped', '#seq-start': 4, '#seq-end': 5},\n",
      " {'#': 'over', '#seq-start': 5, '#seq-end': 6},\n",
      " {'#': 'the', '#seq-start': 6, '#seq-end': 7},\n",
      " {'#': 'lazy', '#seq-start': 7, '#seq-end': 8},\n",
      " {'#': 'dog', '#seq-start': 8, '#seq-end': 9}]\n"
     ]
    }
   ],
   "source": [
    "rules = [\n",
    "\n",
    "    {\"#when\": {\"text\": \"?sentence\"},\n",
    "     \"#then\": [{\"#tokenize\": \"?sentence\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "engine.clear_rules()\n",
    "engine.add_rules(rules)\n",
    "\n",
    "results = engine.process({\"text\": \"the quick brown fox jumped over the lazy dog\"})\n",
    "pprint.pprint(results, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping the Tokens in Assertions\n",
    "\n",
    "Great - now we have generated an assertion for each token (word) in the sentence.\n",
    "\n",
    "What we need now is a way to tell the engine what to do with those assertions, to be a little more specific in how we want to assert them for other rules to pick them up.\n",
    "\n",
    "The #tokenize command has a way to do this, by using the \"assert\" attribute on the command. Within the \"assert\" attribute, you can specify how you want each generated token to be asserted by using a template. To insert the original token in this template, use the \"#\" value and the #tokenize function will insert the token in that spot.\n",
    "\n",
    "In the example below, the #tokenize command will assert each token as a #lookup command, inserting the token into a \"lemma\" attribute within a new fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'#lookup': {'lemma': 'the'}, '#seq-start': 0, '#seq-end': 1},\n",
      " {'#lookup': {'lemma': 'quick'}, '#seq-start': 1, '#seq-end': 2},\n",
      " {'#lookup': {'lemma': 'brown'}, '#seq-start': 2, '#seq-end': 3},\n",
      " {'#lookup': {'lemma': 'fox'}, '#seq-start': 3, '#seq-end': 4},\n",
      " {'#lookup': {'lemma': 'jumped'}, '#seq-start': 4, '#seq-end': 5},\n",
      " {'#lookup': {'lemma': 'over'}, '#seq-start': 5, '#seq-end': 6},\n",
      " {'#lookup': {'lemma': 'the'}, '#seq-start': 6, '#seq-end': 7},\n",
      " {'#lookup': {'lemma': 'lazy'}, '#seq-start': 7, '#seq-end': 8},\n",
      " {'#lookup': {'lemma': 'dog'}, '#seq-start': 8, '#seq-end': 9}]\n"
     ]
    }
   ],
   "source": [
    "rules = [\n",
    "\n",
    "    {\"#when\": {\"text\": \"?sentence\"},\n",
    "     \"#then\": [{\"#tokenize\": \"?sentence\", \"assert\": {\"#lookup\": {\"lemma\": \"#\"}}}]\n",
    "    }\n",
    "]\n",
    "\n",
    "engine.clear_rules()\n",
    "engine.add_rules(rules)\n",
    "\n",
    "results = engine.process({\"text\": \"the quick brown fox jumped over the lazy dog\"})\n",
    "pprint.pprint(results, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing the Tokens with Information from other Facts\n",
    "\n",
    "The #lookup command can now take each token and find the matching fact in the engine's KB and then assert that token back into the engine. The effect of this is that you can enhance facts that have partial information, such as the tokens generated with the #tokenize function, with information available elsewhere in your KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lemma': 'the', 'cat': 'det', '#seq-start': 0, '#seq-end': 1},\n",
      " {'lemma': 'quick', 'cat': 'adj', '#seq-start': 1, '#seq-end': 2},\n",
      " {'lemma': 'brown', 'cat': 'adj', '#seq-start': 2, '#seq-end': 3},\n",
      " {'lemma': 'fox', 'cat': 'noun', '#seq-start': 3, '#seq-end': 4},\n",
      " {'lemma': 'jumped', 'cat': 'verb', '#seq-start': 4, '#seq-end': 5},\n",
      " {'lemma': 'over', 'cat': 'prep', '#seq-start': 5, '#seq-end': 6},\n",
      " {'lemma': 'the', 'cat': 'det', '#seq-start': 6, '#seq-end': 7},\n",
      " {'lemma': 'lazy', 'cat': 'adj', '#seq-start': 7, '#seq-end': 8},\n",
      " {'lemma': 'dog', 'cat': 'noun', '#seq-start': 8, '#seq-end': 9}]\n"
     ]
    }
   ],
   "source": [
    "rules = [\n",
    "  \n",
    "    {\"lemma\": \"the\", \"cat\": \"det\"},\n",
    "    {\"lemma\": \"quick\", \"cat\": \"adj\"},\n",
    "    {\"lemma\": \"brown\", \"cat\": \"adj\"},\n",
    "    {\"lemma\": \"fox\", \"cat\": \"noun\"},\n",
    "    {\"lemma\": \"jumped\", \"cat\": \"verb\"},\n",
    "    {\"lemma\": \"over\", \"cat\": \"prep\"},\n",
    "    {\"lemma\": \"lazy\", \"cat\": \"adj\"},\n",
    "    {\"lemma\": \"dog\", \"cat\": \"noun\"}\n",
    "]\n",
    "\n",
    "engine.add_rules(rules)\n",
    "\n",
    "results = engine.process({\"text\": \"the quick brown fox jumped over the lazy dog\"})\n",
    "pprint.pprint(results, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step - Detecting Sequences\n",
    "\n",
    "Now that we've tokenized a sentence and performed a lookup on each token to return an enhanced version of each, we can use this information to look for sequences of terms. Sequences are key to parsing natural language, such as detecting a noun phrase from a sequence of adjectives followed by a noun."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2131daa06b04808fa66558f781445fffc86a65c38e9ddc639756511472fbc9cd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('thoughts-dev': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
